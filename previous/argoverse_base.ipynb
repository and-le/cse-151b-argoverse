{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0+cu92'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File system\n",
    "import os, os.path \n",
    "import pickle\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "# Workflow\n",
    "import random\n",
    "import tqdm\n",
    "import tqdm.notebook\n",
    "\n",
    "# Computation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Data visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':100})\n",
    "sns.set_context('notebook')\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys to the pickle objects\n",
    "CITY = 'city'\n",
    "LANE = 'lane'\n",
    "LANE_NORM = 'lane_norm'\n",
    "SCENE_IDX = 'scene_idx'\n",
    "AGENT_ID = 'agent_id'\n",
    "P_IN = 'p_in'\n",
    "V_IN = 'v_in'\n",
    "P_OUT = 'p_out'\n",
    "V_OUT = 'v_out'\n",
    "CAR_MASK = 'car_mask'\n",
    "TRACK_ID = 'track_id'\n",
    "\n",
    "# Encode Miami as 0, Pittsburgh as 1\n",
    "MIA = 'MIA'\n",
    "PIT = 'PIT'  \n",
    "CITY_MAP = {MIA: 0, PIT: 1}\n",
    "\n",
    "# Transformed data keys\n",
    "LANE_IN = 'closest_lanes_in'\n",
    "NORM_IN = 'closest_lane_norms_in'\n",
    "LANE_OUT = 'closest_lanes_out'\n",
    "NORM_OUT = 'closest_lane_norms_out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset variables\n",
    "VALIDATION_PATH = './val_data/'\n",
    "ORIGINAL_PATH = './original_train_data/'\n",
    "TRANSFORMED_TRAIN_PATH = './transformed_train_data'\n",
    "TRANSFORMED_VAL_PATH = './transformed_val_data'\n",
    "\n",
    "\n",
    "train_path = TRANSFORMED_TRAIN_PATH\n",
    "val_path = TRANSFORMED_VAL_PATH\n",
    "\n",
    "# Path to model predictions on test set\n",
    "PREDICTION_PATH = './my_submission.csv'\n",
    "# Header of predictions CSV file\n",
    "CSV_HEADER = ['ID,'] + ['v' + str(i) + ',' for i in range(1, 60)] + ['v60', '\\n']\n",
    "\n",
    "# Get list of all training file names\n",
    "original_files = glob(os.path.join(train_path, '*'))\n",
    "\n",
    "# 80-20 train-test split\n",
    "train_files = random.sample(original_files, int(len(original_files) * 0.8))\n",
    "test_files = list(set(original_files) - set(train_files))\n",
    "\n",
    "# Validation\n",
    "val_files = glob(os.path.join(val_path, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Control variables\n",
    "USE_ONE_AGENT = True\n",
    "PREDICT_ONE_AGENT = True  # controls whether 60-agent inputs are mapped to just the target agent's outputs\n",
    "\n",
    "NUM_AGENTS = 1 if USE_ONE_AGENT else 60\n",
    "IN_LEN = 19\n",
    "OUT_LEN = 30\n",
    "\n",
    "# Controls how many features to use\n",
    "N_FEAT_IN = 4\n",
    "N_FEAT_OUT = 2\n",
    "\n",
    "# Batch variables\n",
    "BATCH_SIZE_TRAIN = 64\n",
    "BATCH_SIZE_TEST = BATCH_SIZE_TRAIN\n",
    "BATCH_SIZE_VAL = 32\n",
    "N_WORKERS = 4\n",
    "# Windows doesn't support anything but N_WORKERS = 0 for the DataLoader\n",
    "if 'win' in sys.platform:\n",
    "    N_WORKERS = 0\n",
    "N_WORKERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, files):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.files = files\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.files[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_train_test(batch):\n",
    "    \"\"\" \n",
    "    Custom collate_fn function to be used for DataLoader.    \n",
    "    The input tensor is organized as 19 rows, where each row has 4 columns: px, py, vx, vy\n",
    "    \n",
    "    The output tensor can be organized in 2 ways:\n",
    "    A) The output tensor is organized as 120 rows, where each row has 1 column. \n",
    "    Each contiguous sequence of 4 elements is px, py, vx, vy\n",
    "    \n",
    "    B) The input tensor is organized as 30 rows, where each row has 4 columns: px, py, vx, vy\n",
    "    \"\"\"    \n",
    "    inp = []        \n",
    "    out = []\n",
    "    agent_idxs = []\n",
    "    scene_idxs = []\n",
    "    city_ids = []\n",
    "    for scene in batch:\n",
    "        # Get the target agent id\n",
    "        agent_id = scene[AGENT_ID]        \n",
    "        # Get the matrix of all agents\n",
    "        track_id = scene[TRACK_ID]        \n",
    "        # Get the location of the target agent in the matrix\n",
    "        idx = np.nonzero(track_id[:, 0] == agent_id)[0][0]\n",
    "        \n",
    "        # Number of time steps in the input and output sequences\n",
    "        inlen = scene[P_IN].shape[1]\n",
    "        outlen = scene[P_OUT].shape[1]\n",
    "        \n",
    "        # Aliases of scene variables for convenience\n",
    "        pin, pout, vin, vout = scene[P_IN], scene[P_OUT], scene[V_IN], scene[V_OUT]\n",
    "        lanein, laneout, normin, normout = scene[LANE_IN], scene[LANE_OUT], scene[NORM_IN], scene[NORM_OUT]\n",
    "        \n",
    "        # Use these lines to include only the target agent.\n",
    "        # Training with just one agent is faster and preferred for experimentation\n",
    "        if USE_ONE_AGENT:  \n",
    "            if scene[CITY] == MIA:\n",
    "                cities = np.zeros((inlen, 1)) \n",
    "            else:\n",
    "                cities = np.ones((inlen, 1)) \n",
    "\n",
    "            # Use this line to include all input features\n",
    "#             inp_tens = np.concatenate((pin[idx], vin[idx], lanein[idx], normin[idx], cities), axis=1)    \n",
    "\n",
    "            # Lane and norm only\n",
    "#             inp_tens = np.concatenate((pin[idx], vin[idx], lanein[idx], normin[idx]), axis=1) \n",
    "\n",
    "            # Lane  only\n",
    "#             inp_tens = np.concatenate((pin[idx], vin[idx], lanein[idx]), axis=1) \n",
    "\n",
    "            # Only include cities\n",
    "#             inp_tens = np.concatenate((pin[idx], vin[idx], cities), axis=1)     \n",
    "\n",
    "            # Only include pos/vel\n",
    "#             inp_tens = np.concatenate((pin[idx], vin[idx]), axis=1)\n",
    "\n",
    "\n",
    "            # All output features\n",
    "#             out_tens = np.concatenate((pout[idx], vout[idx], laneout[idx], normout[idx]), axis=1)\n",
    "            # Only include output position and velocity\n",
    "#             out_tens = np.concatenate((pout[idx], vout[idx]), axis=1)   \n",
    "        \n",
    "            out_tens = pout[idx]\n",
    "            \n",
    "        # Use these lines to include all 60 agents, even the dummy agents.\n",
    "        # Training with all 60 agents is much slower than with just one agent.\n",
    "        else:\n",
    "            num_agents = pin.shape[0]\n",
    "            if scene[CITY] == MIA:\n",
    "                cities = np.zeros((num_agents, inlen, 1)) \n",
    "            else:\n",
    "                cities = np.ones((num_agents, inlen, 1))  \n",
    "                \n",
    "            # Include all features\n",
    "#             inp_tens = np.concatenate((pin, vin, lanein, normin, cities), axis=2)  \n",
    "            # Lane only\n",
    "#             inp_tens = np.concatenate((pin, vin, lanein, normin), axis=2)   \n",
    "            # Include only cities\n",
    "            inp_tens = np.concatenate((pin, vin, cities), axis=2) \n",
    "            # Include only pos/vel\n",
    "#             inp_tens = np.concatenate((pin, vin), axis=2)\n",
    "            \n",
    "            # Makes predictions for only the target agent\n",
    "            if PREDICT_ONE_AGENT:\n",
    "                # All features\n",
    "#                 out_tens = np.concatenate((pout[idx], vout[idx], laneout[idx], normout[idx]), axis=1)                \n",
    "                # Pos/vel\n",
    "                out_tens = np.concatenate((pout[idx], vout[idx]), axis=1)\n",
    "\n",
    "            # Makes predictions for all 60 agents\n",
    "            else:\n",
    "                # All features\n",
    "#                 out_tens = np.concatenate((pout, vout, laneout, normout), axis=2)                \n",
    "                # pv\n",
    "                out_tens = np.concatenate((pout, vout), axis=2) \n",
    "        \n",
    "        inp.append(inp_tens)\n",
    "        out.append(out_tens)        \n",
    "        scene_idxs.append(scene[SCENE_IDX]) \n",
    "        agent_idxs.append(idx)\n",
    "        city_ids.append(CITY_MAP[scene[CITY]])\n",
    "\n",
    "    inp = torch.FloatTensor(inp)\n",
    "    out = torch.FloatTensor(out)\n",
    "    return [inp, out, scene_idxs, agent_idxs, city_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_val(batch):\n",
    "    \"\"\" \n",
    "    Custom collate_fn for validation dataset. The validation data do not contain output values.   \n",
    "    The input tensor is organized as 19 rows, where each row has 4 columns: px, py, vx, vy\n",
    "    \"\"\"   \n",
    "    inp = []\n",
    "    scene_idxs = []\n",
    "    agent_idxs = []\n",
    "    city_ids = []\n",
    "    \n",
    "    for scene in batch:\n",
    "        # Get the target agent id\n",
    "        agent_id = scene[AGENT_ID]        \n",
    "        # Get the matrix of all agents\n",
    "        track_id = scene[TRACK_ID]        \n",
    "        # Get the location of the target agent in the matrix\n",
    "        idx = np.nonzero(track_id[:, 0] == agent_id)[0][0]\n",
    "        \n",
    "        inlen = scene[P_IN].shape[1]\n",
    "        \n",
    "        # Aliases of scene variables for convenience\n",
    "        pin, vin = scene[P_IN], scene[V_IN]\n",
    "        lanein, normin = scene[LANE_IN], scene[NORM_IN]\n",
    "        num_agents = scene[P_IN].shape[0]\n",
    "        \n",
    "        # Use these lines to include only the target agent.\n",
    "        if USE_ONE_AGENT:\n",
    "            if scene[CITY] == MIA:\n",
    "                cities = np.zeros((inlen, 1)) \n",
    "            else:\n",
    "                cities = np.ones((inlen, 1)) \n",
    "            # All\n",
    "#             inp_tens = np.concatenate((pin[idx], vin[idx], lanein[idx], normin[idx], cities), axis=1)\n",
    "            # Lane and norm only\n",
    "#             inp_tens = np.concatenate((pin[idx], vin[idx], lanein[idx], normin[idx]), axis=1)\n",
    "\n",
    "            # Lane  only\n",
    "#             inp_tens = np.concatenate((pin[idx], vin[idx], lanein[idx]), axis=1) \n",
    "        \n",
    "            # Cities\n",
    "#             inp_tens = np.concatenate((pin[idx], vin[idx], cities), axis=1)  \n",
    "            # Pos/vel\n",
    "            inp_tens = np.concatenate((pin[idx], vin[idx]), axis=1)  \n",
    "\n",
    "        # Use these lines to include all 60 agents\n",
    "        else:\n",
    "            if scene[CITY] == MIA:\n",
    "                cities = np.zeros((num_agents, inlen, 1)) \n",
    "            else:\n",
    "                cities = np.ones((num_agents, inlen, 1))\n",
    "            # All features\n",
    "#             inp_tens = np.concatenate( (pin, vin, lanein, normin, cities), axis=2)       \n",
    "            # Lane only\n",
    "#             inp_tens = np.concatenate( (pin, vin, lanein, normin), axis=2)\n",
    "            # Cities\n",
    "            inp_tens = np.concatenate( (pin, vin, cities), axis=2)             \n",
    "            # pv\n",
    "#             inp_tens = np.concatenate( (pin, vin), axis=2)  \n",
    "        \n",
    "        inp.append(inp_tens)        \n",
    "        scene_idxs.append(scene[SCENE_IDX])\n",
    "        agent_idxs.append(idx)\n",
    "        city_ids.append(CITY_MAP[scene[CITY]])        \n",
    "        \n",
    "    inp = torch.FloatTensor(inp)    \n",
    "    return [inp, scene_idxs, agent_idxs, city_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliaze datasets and loaders\n",
    "\n",
    "train_dataset = ArgoverseDataset(train_files)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, \n",
    "                                           shuffle=True, collate_fn=collate_train_test, \n",
    "                                           num_workers=N_WORKERS)\n",
    "test_dataset = ArgoverseDataset(test_files)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST, \n",
    "                                          shuffle=False, collate_fn=collate_train_test, \n",
    "                                          num_workers=N_WORKERS)\n",
    "val_dataset = ArgoverseDataset(val_files)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE_VAL, \n",
    "                                         shuffle=False, collate_fn=collate_val,\n",
    "                                         num_workers=N_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-13-148a6122f974>\", line 56, in collate_train_test\n    inp_tens = np.concatenate((pin[idx], vin[idx], vout[idx]), axis=1)\n  File \"<__array_function__ internals>\", line 6, in concatenate\nValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 19 and the array at index 2 has size 30\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f87fad8f0a4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Look at one data sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscene_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcities\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-13-148a6122f974>\", line 56, in collate_train_test\n    inp_tens = np.concatenate((pin[idx], vin[idx], vout[idx]), axis=1)\n  File \"<__array_function__ internals>\", line 6, in concatenate\nValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 19 and the array at index 2 has size 30\n"
     ]
    }
   ],
   "source": [
    "# Look at one data sample      \n",
    "for _, (data, target, agent_idxs, scene_idxs, cities) in enumerate(train_loader):\n",
    "    print(data.shape)\n",
    "    print(target.shape) \n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at one data sample      \n",
    "for _, (data, agent_idxs, scene_idxs, masks) in enumerate(val_loader):\n",
    "    print(data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model into training mode\n",
    "    model.train()    \n",
    "    \n",
    "    # Define the loss function.\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')\n",
    "    total_loss = 0    \n",
    "    for i in range(epoch):\n",
    "        h = c = None # Hidden states      \n",
    "\n",
    "        iterator = tqdm.notebook.tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for _, batch in enumerate(iterator):\n",
    "            data, target, scene_idxs, agent_idxs, cities = batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()            \n",
    "    \n",
    "            out = model(data)\n",
    "            # Compute the loss            \n",
    "            loss = torch.sqrt(criterion(out, target))            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the progress bar for tqdm\n",
    "            iterator.set_postfix(train_loss=loss.item())\n",
    "            \n",
    "    return (total_loss * BATCH_SIZE_TRAIN) / len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()    \n",
    "    criterion = torch.nn.MSELoss(reduction='mean')    \n",
    "    iterator = tqdm.notebook.tqdm(test_loader, total=int(len(test_loader)))\n",
    "    total_loss = 0\n",
    "    \n",
    "    for _, batch in enumerate(iterator):\n",
    "        data, target, scene_idxs, agent_idxs, masks = batch\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            out = model(data)            \n",
    "            loss = torch.sqrt(criterion(out, target))        \n",
    "            total_loss += loss.item()\n",
    "                \n",
    "            iterator.set_postfix(test_loss=loss.item())\n",
    "        \n",
    "    return (total_loss * BATCH_SIZE_TEST) / len(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, DEVICE, train_loader, optimizer, NUM_EPOCH):\n",
    "    for t in range(1, NUM_EPOCH + 1):\n",
    "        train_loss = train(model, DEVICE, train_loader, optimizer, 1)\n",
    "        test_loss = test(model, DEVICE, test_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print(f'Epoch {t}: train_loss = {train_loss}, test_loss = {test_loss}')         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, val_loader, path):\n",
    "    \"\"\"\n",
    "    path: path to csv file to write predictions\n",
    "    \"\"\"\n",
    "    model.eval()   \n",
    "    \n",
    "    # Prep the output file\n",
    "    with open(PREDICTION_PATH, \"w\") as csv_file:\n",
    "        # Clear the csv file before appending data to it\n",
    "        csv_file.truncate()\n",
    "        # Write the header to the csv file\n",
    "        csv_file.writelines(CSV_HEADER)    \n",
    "    \n",
    "    # Make predictions\n",
    "    with open(path, \"a\") as pred_file:        \n",
    "        iterator = tqdm.notebook.tqdm(val_loader, total=int(len(val_loader)))\n",
    "        \n",
    "        for _, batch in enumerate(iterator):\n",
    "            data, scene_idxs, agent_idxs, cities = batch\n",
    "            data = data.to(device) \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "                # Convert the Tensor from GPU -> CPU -> NumPy array\n",
    "                np_out = output.cpu().detach().numpy()\n",
    "                \n",
    "                # Store only the predictions for the target agent and keep the positions, not the velocities\n",
    "                batch_size = np_out.shape[0]\n",
    "                \n",
    "                pred = np.zeros((batch_size, 60))\n",
    "                if PREDICT_ONE_AGENT:\n",
    "                    # The output should be a (batch size, time steps, num features out) tensor \n",
    "                    # where the first two features are the out position x, y\n",
    "                    for i in range(batch_size):\n",
    "                        pred[i] = np_out[i, :, :2].flatten()\n",
    "                else:\n",
    "                    # The output should be a (batch size, num agents, time steps, num features out) tensor \n",
    "                    # where the first two features are the input position and the output position\n",
    "                    for i in range(batch_size):\n",
    "                        idx = agent_idxs[i]\n",
    "                        pred[i] = np_out[i, idx, :, :2].flatten()                    \n",
    "\n",
    "                # Form comma-separated string\n",
    "                s = []\n",
    "                for i in range(pred.shape[0]):\n",
    "                    s.append(','.join([str(scene_idxs[i])] + [str(v) for v in pred[i]]) + '\\n')\n",
    "\n",
    "                # Write data to file\n",
    "                pred_file.writelines(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Neural Network class - linear regression\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(ArgoNet, self).__init__() \n",
    "        \n",
    "        self.device = device        \n",
    "        # Linear regression for 1 agent     \n",
    "        if PREDICT_ONE_AGENT:\n",
    "            self.fc = nn.Linear(NUM_AGENTS * IN_LEN * N_FEAT_IN, 1 * OUT_LEN * N_FEAT_OUT)\n",
    "        else:\n",
    "            self.fc = nn.Linear(NUM_AGENTS * IN_LEN * N_FEAT_IN, NUM_AGENTS * OUT_LEN * N_FEAT_OUT)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # flatten result\n",
    "        x = self.fc(x)            \n",
    "        if PREDICT_ONE_AGENT:\n",
    "            x = x.view(x.size(0), OUT_LEN, N_FEAT_OUT)\n",
    "        else:\n",
    "            x = x.view(x.size(0), NUM_AGENTS, OUT_LEN, N_FEAT_OUT)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ArgoNet(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Neural Network class - 1 hidden layer\n",
    "#     \"\"\"\n",
    "#     def __init__(self, device):\n",
    "#         super(ArgoNet, self).__init__() \n",
    "        \n",
    "#         self.device = device \n",
    "#         self.hidden_size = IN_LEN * N_FEAT_IN * 2\n",
    "#         if PREDICT_ONE_AGENT:\n",
    "#             self.fc = nn.Sequential(\n",
    "#                 nn.Linear(NUM_AGENTS * IN_LEN * N_FEAT_IN, 1 * self.hidden_size),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(self.hidden_size, 1 * OUT_LEN * N_FEAT_OUT)\n",
    "#             )\n",
    "#         else:\n",
    "#             self.fc = nn.Linear(NUM_AGENTS * IN_LEN * N_FEAT_IN, NUM_AGENTS * OUT_LEN * N_FEAT_OUT)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = x.view(x.size(0), -1)  # flatten result\n",
    "#         x = self.fc(x)            \n",
    "#         if PREDICT_ONE_AGENT:\n",
    "#             x = x.view(x.size(0), OUT_LEN, N_FEAT_OUT)\n",
    "#         else:\n",
    "#             x = x.view(x.size(0), NUM_AGENTS, OUT_LEN, N_FEAT_OUT)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ArgoNet(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Neural Network class - 2 hidden layers\n",
    "#     \"\"\"\n",
    "#     def __init__(self, device):\n",
    "#         super(ArgoNet, self).__init__() \n",
    "        \n",
    "#         self.device = device \n",
    "#         # Linear regression for 1 agent     \n",
    "#         if PREDICT_ONE_AGENT:\n",
    "#             self.fc = nn.Sequential(\n",
    "#                 nn.Linear(NUM_AGENTS * IN_LEN * N_FEAT_IN, 1 * 512),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(512, 1024),\n",
    "#                 nn.ReLU(),\n",
    "#                 nn.Linear(1024, 1 * OUT_LEN * N_FEAT_OUT)\n",
    "#             )\n",
    "#         else:\n",
    "#             self.fc = nn.Linear(NUM_AGENTS * IN_LEN * N_FEAT_IN, NUM_AGENTS * OUT_LEN * N_FEAT_OUT)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = x.view(x.size(0), -1)  # flatten result\n",
    "#         x = self.fc(x)            \n",
    "#         if PREDICT_ONE_AGENT:\n",
    "#             x = x.view(x.size(0), OUT_LEN, N_FEAT_OUT)\n",
    "#         else:\n",
    "#             x = x.view(x.size(0), NUM_AGENTS, OUT_LEN, N_FEAT_OUT)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to save and reload a model\n",
    "MODEL_STATE = 'model_state_dict'\n",
    "OPTIMIZER_STATE = 'optimizer_state_dict'\n",
    "EPOCH_STATE = 'epoch'\n",
    "LOSS_STATE = 'loss'\n",
    "BATCH_STATE = 'batch'\n",
    "\n",
    "def save_model(path, model_state_dict, optimizer_state_dict, epoch, loss, batch):\n",
    "    to_save = {\n",
    "        MODEL_STATE: model_state_dict,\n",
    "        OPTIMIZER_STATE: optimizer_state_dict,\n",
    "        EPOCH_STATE: epoch,\n",
    "        LOSS_STATE: loss,\n",
    "        BATCH_STATE: batch\n",
    "    }\n",
    "    torch.save(to_save, path)\n",
    "    \n",
    "def load_model(path, model_to_load, optimizer_to_load):\n",
    "    checkpoint = torch.load(path)\n",
    "    model_to_load.load_state_dict(checkpoint[MODEL_STATE])\n",
    "    optimizer_to_load.load_state_dict(checkpoint[OPTIMIZER_STATE])\n",
    "    return checkpoint[EPOCH_STATE], checkpoint[LOSS_STATE], checkpoint[BATCH_STATE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters is 6900\n"
     ]
    }
   ],
   "source": [
    "model = ArgoNet(DEVICE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# learning_rate = 1e-1\n",
    "# momentum = 0.9\n",
    "# weight_decay = 1\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())   \n",
    "print(f\"Number of model parameters is {num_params}\")\n",
    "NUM_EPOCH = 3\n",
    "\n",
    "# used for visualizing the loss\n",
    "train_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these two lines to save a model\n",
    "# save_model('two_hid_one_agent_ep6.tar', model.state_dict(), optimizer.state_dict(), NUM_EPOCH, \n",
    "#            (train_losses[-1], test_losses[-1]), BATCH_SIZE_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload a model\n",
    "# model = ArgoNet(DEVICE).to(DEVICE)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# load_model('one_hid_one_agent.tar', model, optimizer)\n",
    "# train_losses.clear()\n",
    "# test_losses.clear()\n",
    "# num_params = sum(p.numel() for p in model.parameters())   \n",
    "# print(f\"Number of model parameters is {num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14da20a323c4ad4884a952d8363d439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2575.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-13-148a6122f974>\", line 56, in collate_train_test\n    inp_tens = np.concatenate((pin[idx], vin[idx], vout[idx]), axis=1)\n  File \"<__array_function__ internals>\", line 6, in concatenate\nValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 19 and the array at index 2 has size 30\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3899f4f98d21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-744ed2ee88ce>\u001b[0m in \u001b[0;36mtrain_test\u001b[0;34m(model, DEVICE, train_loader, optimizer, NUM_EPOCH)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-b7c7eac482b9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscene_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Caught ValueError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 47, in fetch\n    return self.collate_fn(data)\n  File \"<ipython-input-13-148a6122f974>\", line 56, in collate_train_test\n    inp_tens = np.concatenate((pin[idx], vin[idx], vout[idx]), axis=1)\n  File \"<__array_function__ internals>\", line 6, in concatenate\nValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 19 and the array at index 2 has size 30\n"
     ]
    }
   ],
   "source": [
    "train_test(model, DEVICE, train_loader, optimizer, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653cdc6022ce4ad29057e56998e618e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "validate(model, DEVICE, val_loader, PREDICTION_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(losses):\n",
    "    \"\"\"\n",
    "    Plots the losses over each training iteration. \n",
    "    Assumes that each element of the 'losses' list corresponds to the loss after each batch of train()\n",
    "    \"\"\"\n",
    "    t_iter = np.arange(1, len(losses) + 1, 1, dtype=int)\n",
    "    ax = sns.scatterplot(x=t_iter, y=losses, alpha=0.5)    \n",
    "    ax.set_xlabel('Batch iteration number')\n",
    "    ax.set_ylabel('Root-mean-square loss')\n",
    "    ax.set_title('Batch Iteration vs. Root-Mean-Square Loss')\n",
    "    plt.savefig('lossViter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_loss(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, device, loader):\n",
    "    \"\"\"\n",
    "    Compares some randomly selected data samples to the model's predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of data\n",
    "    _, (inp, out, scene_idxs, agent_idxs, masks) = next(enumerate(loader))\n",
    "    \n",
    "    # Move tensors to chosen device\n",
    "    inp, out = inp.to(device), out.to(device)\n",
    "    \n",
    "    # Sample number\n",
    "    i = 0\n",
    "    \n",
    "    # Scene idx\n",
    "    scene_idx = scene_idxs[i]\n",
    "        \n",
    "    # Get contiguous arrays of the ground truth output positions\n",
    "    truth = target[i].cpu().detach().numpy()\n",
    "    x = truth[:, 0]\n",
    "    y = truth[:, 1] \n",
    "        \n",
    "    # Get contiguous arrays of the prediction output positions\n",
    "    output = model(inp)    \n",
    "    pred = output[i].cpu().detach().numpy()\n",
    "    xh = pred[:, 0]\n",
    "    yh = pred[:, 0]    \n",
    "    \n",
    "    # Plot the ground truth and prediction positions\n",
    "    fig, (ax) = plt.subplots(nrows=1, ncols=1, figsize=(3, 3))\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('Scene ' + str(scene_idx))\n",
    "    ax.scatter(x, y, label='Ground Truth')\n",
    "    ax.scatter(xh, yh, label='Prediction')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(model, DEVICE, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
