{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1+cu92'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Workflow\n",
    "import random\n",
    "import tqdm\n",
    "import tqdm.notebook\n",
    "\n",
    "# Computation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Data visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set(rc={\"figure.dpi\":100, 'savefig.dpi':100})\n",
    "sns.set_context('notebook')\n",
    "random.seed(0)\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys\n",
    "SCENE_IDX = 'scene_idx'\n",
    "P_IN = 'p_in'\n",
    "V_IN = 'v_in'\n",
    "P_OUT = 'p_out'\n",
    "V_OUT = 'v_out'\n",
    "L_IN = 'closest_lane_in'\n",
    "N_IN = 'closest_norm_in'\n",
    "\n",
    "# Header of predictions CSV file\n",
    "CSV_HEADER = ['ID,'] + ['v' + str(i) + ',' for i in range(1, 60)] + ['v60', '\\n']\n",
    "\n",
    "TRAIN_TEST_RATIO = 0.99\n",
    "BATCH_SIZE_TRAIN = 256\n",
    "BATCH_SIZE_TEST = 256\n",
    "BATCH_SIZE_VAL = 64\n",
    "N_WORKERS = 2\n",
    "\n",
    "MODEL_STATE = 'model_state_dict'\n",
    "OPTIMIZER_STATE = 'optimizer_state_dict'\n",
    "EPOCH_STATE = 'epoch'\n",
    "LOSS_STATE = 'loss'\n",
    "BATCH_STATE = 'batch'\n",
    "SCHEDULER_STATE = 'scheduler_state_dict'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_data(PIN, VIN, POUT, LIN=None, NIN=None):\n",
    "    # Build the dataset\n",
    "    original_data = []\n",
    "    if LIN is None:\n",
    "        iterator = tqdm.notebook.tqdm(zip(PIN.iterrows(), VIN.iterrows(), POUT.iterrows()), total=len(PIN))\n",
    "        for (i1, r1), (_, r2), (_, r3) in iterator:\n",
    "            d = {\n",
    "                SCENE_IDX: r1[0], \n",
    "                P_IN: r1.to_numpy()[1:].reshape(19, 2),\n",
    "                V_IN: r2.to_numpy()[1:].reshape(19, 2),\n",
    "                P_OUT: r3.to_numpy()[1:].reshape(30, 2)\n",
    "            }\n",
    "            original_data.append(d)\n",
    "    else:\n",
    "        iterator = tqdm.notebook.tqdm(zip(PIN.iterrows(), VIN.iterrows(), POUT.iterrows(), \n",
    "                                          LIN.iterrows(), NIN.iterrows()), total=len(PIN))\n",
    "        for (i1, r1), (_, r2), (_, r3), (_, r4), (_, r5) in iterator:\n",
    "            d = {\n",
    "                SCENE_IDX: r1[0], \n",
    "                P_IN: r1.to_numpy()[1:].reshape(19, 2),\n",
    "                V_IN: r2.to_numpy()[1:].reshape(19, 2),\n",
    "                P_OUT: r3.to_numpy()[1:].reshape(30, 2),\n",
    "                L_IN: r4.to_numpy()[1:].reshape(19, 2),\n",
    "                N_IN: r5.to_numpy()[1:].reshape(19, 2)\n",
    "            }\n",
    "            original_data.append(d)\n",
    "    \n",
    "    return original_data\n",
    "\n",
    "def get_train_test_data(original_data, ratio):\n",
    "    # Split into train and test\n",
    "    random.shuffle(original_data)\n",
    "    TRAIN_SIZE = int(len(original_data) * ratio)\n",
    "    train_data = original_data[:TRAIN_SIZE]\n",
    "    test_data = original_data[TRAIN_SIZE:]\n",
    "    return train_data, test_data\n",
    "    \n",
    "def get_val_data(PIN, VIN, LIN=None, NIN=None):\n",
    "    val_data = []\n",
    "    if LIN is None:\n",
    "        iterator = tqdm.notebook.tqdm(zip(PIN.iterrows(), VIN.iterrows()), total=len(PIN))\n",
    "        for (i1, r1), (_, r2) in iterator:\n",
    "            d = {\n",
    "                SCENE_IDX: r1[0], \n",
    "                P_IN: r1.to_numpy()[1:].reshape(19, 2),\n",
    "                V_IN: r2.to_numpy()[1:].reshape(19, 2)\n",
    "            }\n",
    "            val_data.append(d)\n",
    "    else:\n",
    "        iterator = tqdm.notebook.tqdm(zip(PIN.iterrows(), VIN.iterrows(), LIN.iterrows(), NIN.iterrows()), total=len(PIN))\n",
    "        for (i1, r1), (_, r2), (_, r3), (_, r4), in iterator:\n",
    "            d = {\n",
    "                SCENE_IDX: r1[0], \n",
    "                P_IN: r1.to_numpy()[1:].reshape(19, 2),\n",
    "                V_IN: r2.to_numpy()[1:].reshape(19, 2),\n",
    "                L_IN: r3.to_numpy()[1:].reshape(19, 2),\n",
    "                N_IN: r4.to_numpy()[1:].reshape(19, 2)\n",
    "            }\n",
    "            val_data.append(d)\n",
    "    return val_data\n",
    "\n",
    "def get_data(kind, features):\n",
    "    # Train/test data\n",
    "    LIN = None\n",
    "    NIN = None\n",
    "    if kind == 'single':\n",
    "        PIN = pd.read_csv('./targ_train_data/pin_train.csv')\n",
    "        VIN = pd.read_csv('./targ_train_data/vin_train.csv')\n",
    "        POUT = pd.read_csv('./targ_train_data/pout_train.csv')\n",
    "\n",
    "    else:    \n",
    "        # All tracked agents\n",
    "        PIN = pd.read_csv('./track_train_data/pin_trainall.csv')\n",
    "        VIN = pd.read_csv('./track_train_data/vin_trainall.csv')\n",
    "        POUT = pd.read_csv('./track_train_data/pout_trainall.csv')\n",
    "        \n",
    "        if features == 'extended':\n",
    "            LIN = pd.read_csv('./track_train_data/lanein.csv')\n",
    "            NIN = pd.read_csv('./track_train_data/normin.csv')\n",
    "\n",
    "        \n",
    "    original_data = get_original_data(PIN, VIN, POUT, LIN, NIN)\n",
    "    train_data, test_data = get_train_test_data(original_data, TRAIN_TEST_RATIO)\n",
    "    \n",
    "    # Validation data\n",
    "    PIN = pd.read_csv('./targ_val_data/pin_val.csv')\n",
    "    VIN = pd.read_csv('./targ_val_data/vin_val.csv')\n",
    "    if features == 'extended':\n",
    "        LIN = pd.read_csv('./targ_val_data/lanein.csv')\n",
    "        NIN = pd.read_csv('./targ_val_data/normin.csv')\n",
    "        \n",
    "    val_data = get_val_data(PIN, VIN, LIN, NIN)\n",
    "    \n",
    "    return train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_train_test(batch):\n",
    "    \"\"\" \n",
    "    Custom collate_fn function to be used for DataLoader.    \n",
    "    The input tensor is organized as 19 rows, where each row has 4 columns: px, py, vx, vy\n",
    "    \n",
    "    The output tensor can be organized in 2 ways:\n",
    "    A) The output tensor is organized as 120 rows, where each row has 1 column. \n",
    "    Each contiguous sequence of 4 elements is px, py, vx, vy\n",
    "    \n",
    "    B) The input tensor is organized as 30 rows, where each row has 4 columns: px, py, vx, vy\n",
    "    \"\"\"    \n",
    "    inp = []     \n",
    "    out = []\n",
    "    scene_idxs = []\n",
    "    offsets = np.zeros((len(batch), 2))\n",
    "    for i, scene in enumerate(batch):      \n",
    "        pin, vin, pout = np.copy(scene[P_IN]), np.copy(scene[V_IN]), np.copy(scene[P_OUT])        \n",
    "        # Normalize position\n",
    "        xs, ys = pin[0, 0], pin[0, 1]       \n",
    "        pin[:, 0] -= xs\n",
    "        pin[:, 1] -= ys\n",
    "        pout[:, 0] -= xs\n",
    "        pout[:, 1] -= ys\n",
    "        \n",
    "        lin, nin = np.copy(scene[L_IN]), np.copy(scene[N_IN])\n",
    "        lin[:, 0] -= xs\n",
    "        lin[:, 1] -= ys\n",
    "        inp_tens = np.concatenate((pin, vin, lin, nin), axis=1) \n",
    "    \n",
    "#         inp_tens = np.concatenate((pin, vin), axis=1)     \n",
    "        out_tens = pout\n",
    "        \n",
    "        inp.append(inp_tens)\n",
    "        out.append(out_tens)  \n",
    "        scene_idxs.append(scene[SCENE_IDX])\n",
    "        offsets[i, 0], offsets[i, 1] = xs, ys\n",
    "\n",
    "    inp = torch.FloatTensor(inp)\n",
    "    out = torch.FloatTensor(out)\n",
    "    return [inp, out, scene_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_val(batch):\n",
    "    \"\"\" \n",
    "    Custom collate_fn for validation dataset. The validation data do not contain output values.   \n",
    "    The input tensor is organized as 19 rows, where each row has 4 columns: px, py, vx, vy\n",
    "    \"\"\"   \n",
    "    inp = []\n",
    "    scene_idxs = []\n",
    "    offsets = np.zeros((len(batch), 2))  # start (x, y) coordinates from normalization\n",
    "    \n",
    "    for i, scene in enumerate(batch):          \n",
    "        pin, vin = np.copy(scene[P_IN]), np.copy(scene[V_IN])\n",
    "        # Normalize position\n",
    "        xs, ys = pin[0, 0], pin[0, 1]      \n",
    "        pin[:, 0] -= xs\n",
    "        pin[:, 1] -= ys\n",
    "        \n",
    "        lin, nin = np.copy(scene[L_IN]), np.copy(scene[N_IN])\n",
    "        lin[:, 0] -= xs\n",
    "        lin[:, 1] -= ys\n",
    "        inp_tens = np.concatenate((pin, vin, lin, nin), axis=1)\n",
    "        \n",
    "#         inp_tens = np.concatenate((pin, vin), axis=1)        \n",
    "        inp.append(inp_tens)        \n",
    "        scene_idxs.append(scene[SCENE_IDX])\n",
    "        offsets[i, 0], offsets[i, 1] = xs, ys\n",
    "    inp = torch.FloatTensor(inp)    \n",
    "    return [inp, scene_idxs, offsets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, train_data):\n",
    "    # Set the model into training mode\n",
    "    model.train()      \n",
    "    # Define the loss function.\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    total_loss = 0    \n",
    "    for i in range(epoch):\n",
    "        iterator = tqdm.notebook.tqdm(train_loader, total=int(len(train_loader)))\n",
    "        for _, batch in enumerate(iterator):\n",
    "            data, target, _ = batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            \n",
    "            out = model(data)\n",
    "            # Compute the loss       \n",
    "            loss = torch.sqrt(criterion(out, target)) \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "            # Update the weights\n",
    "            optimizer.step()    \n",
    "            \n",
    "#             scheduler.step()  # Uncomment if using CyclicLR\n",
    "\n",
    "            # Update the progress bar for tqdm\n",
    "            iterator.set_postfix(train_loss=loss.item())\n",
    "            \n",
    "    return (total_loss * BATCH_SIZE_TRAIN) / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, test_data):\n",
    "    model.eval()    \n",
    "    criterion = torch.nn.MSELoss(reduction='mean') \n",
    "\n",
    "    iterator = tqdm.notebook.tqdm(test_loader, total=int(len(test_loader)))\n",
    "    total_loss = 0    \n",
    "    for _, batch in enumerate(iterator):\n",
    "        data, target, _ = batch\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model(data)            \n",
    "            loss = torch.sqrt(criterion(out, target))        \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            iterator.set_postfix(test_loss=loss.item())\n",
    "                    \n",
    "    return (total_loss * BATCH_SIZE_TEST) / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, device, val_loader, path):\n",
    "    \"\"\"\n",
    "    path: path to csv file to write predictions\n",
    "    \"\"\"\n",
    "    model.eval() \n",
    "    \n",
    "    # Prep the output file\n",
    "    with open(path, \"w\") as csv_file:\n",
    "        # Clear the csv file before appending data to it\n",
    "        csv_file.truncate()\n",
    "        # Write the header to the csv file\n",
    "        csv_file.writelines(CSV_HEADER)    \n",
    "    \n",
    "    # Make predictions\n",
    "    with open(path, \"a\") as pred_file:        \n",
    "        iterator = tqdm.notebook.tqdm(val_loader, total=int(len(val_loader)))\n",
    "        \n",
    "        for _, batch in enumerate(iterator):\n",
    "            data, scene_idxs, offsets = batch\n",
    "            data = data.to(device) \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "                # Convert the Tensor from GPU -> CPU -> NumPy array\n",
    "                np_out = output.cpu().detach().numpy()\n",
    "                \n",
    "                # Store only the predictions for the target agent and keep the positions, not the velocities\n",
    "                batch_size = np_out.shape[0]\n",
    "                \n",
    "                pred = np.zeros((batch_size, 60))\n",
    "                # The output should be a (30 x 2) tensor \n",
    "                # where the first two features are the out position x, y\n",
    "                for i in range(batch_size):\n",
    "                    # Re-scale values\n",
    "                    xs, ys = offsets[i, 0], offsets[i, 1]\n",
    "                    np_out[i, :, 0] += xs\n",
    "                    np_out[i, :, 1] += ys\n",
    "                    pred[i] = np_out[i, :, :2].flatten()                       \n",
    "\n",
    "                # Form comma-separated string\n",
    "                s = []\n",
    "                for i in range(pred.shape[0]):\n",
    "                    s.append(','.join([str(scene_idxs[i])] + [str(v) for v in pred[i]]) + '\\n')\n",
    "\n",
    "                # Write data to file\n",
    "                pred_file.writelines(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ArgoNet(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Neural Network class - linear regression\n",
    "#     \"\"\"\n",
    "#     def __init__(self, device):\n",
    "#         super(ArgoNet, self).__init__() \n",
    "        \n",
    "#         self.device = device        \n",
    "#         # Linear regression for 1 agent     \n",
    "#         self.fc = nn.Linear(19 * 4, 30 * 2)   \n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         z = self.fc(x.view(x.shape[0], -1))\n",
    "#         z = z.view(z.size(0), 30, 2)  \n",
    "#         return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ArgoNet(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     CNN\n",
    "#     \"\"\"\n",
    "#     def __init__(self, device):\n",
    "#         super(ArgoNet, self).__init__() \n",
    "        \n",
    "#         self.device = device        \n",
    "#         # Linear regression for 1 agent \n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv1d(19, 12, 1),\n",
    "#             nn.SELU(),\n",
    "#         )\n",
    "#         self.fc = nn.Linear(12 * 4, 30 * 2)   \n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         z = self.conv(x)\n",
    "#         z = self.fc(z.view(z.shape[0], -1))\n",
    "#         z = z.view(z.size(0), 30, 2)  \n",
    "#         return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    RNN\n",
    "    \"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(ArgoNet, self).__init__() \n",
    "        \n",
    "        self.device = device        \n",
    "        self.hidden_size = 256\n",
    "        self.num_layers = 1\n",
    "        self.bidir = True\n",
    "        self.num_dir = 2 if self.bidir else 1\n",
    "        self.rnn = nn.LSTM(8, self.hidden_size, self.num_layers, batch_first=True, bidirectional=self.bidir)\n",
    "        self.fc = nn.Linear(self.hidden_size * self.num_dir, 30 * 2)   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        z, (h, c) = self.rnn(x)\n",
    "        out = self.fc(z[:, -1, :])\n",
    "        out = out.view(out.size(0), 30, 2)  \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ArgoNet(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     CNN + RNN\n",
    "#     \"\"\"\n",
    "#     def __init__(self, device):\n",
    "#         super(ArgoNet, self).__init__() \n",
    "        \n",
    "#         self.device = device        \n",
    "#         self.hidden_size = 120\n",
    "#         self.num_layers = 1\n",
    "#         self.bidir = True\n",
    "#         self.num_dir = 2 if self.bidir else 1\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv1d(19, 19, 1),\n",
    "#             nn.SELU(),\n",
    "#         )\n",
    "#         self.rnn = nn.LSTM(4, self.hidden_size, self.num_layers, batch_first=True, bidirectional=self.bidir)\n",
    "#         self.fc = nn.Linear(self.hidden_size * self.num_dir, 30 * 2)   \n",
    "\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         z = self.conv(x)\n",
    "#         z, (h, c) = self.rnn(z)\n",
    "#         out = self.fc(z[:, -1, :])\n",
    "#         out = out.view(out.size(0), 30, 2)  \n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ArgoNet(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Encoder-Decoder\n",
    "#     \"\"\"\n",
    "#     def __init__(self, device):\n",
    "#         super(ArgoNet, self).__init__() \n",
    "        \n",
    "#         self.device = device        \n",
    "#         # Linear regression for 1 agent \n",
    "#         self.hidden_size = 120\n",
    "#         self.num_layers = 1\n",
    "#         self.bidir = True\n",
    "#         self.num_dir = 2 if self.bidir else 1\n",
    "#         self.enc = nn.LSTM(4, self.hidden_size, self.num_layers, batch_first=True, bidirectional=self.bidir)\n",
    "#         self.dec = nn.LSTM(4, self.hidden_size, self.num_layers, batch_first=True, bidirectional=self.bidir)\n",
    "#         self.fc = nn.Linear(self.num_dir * self.hidden_size, 1 * 4)   \n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         z, (h, c) = self.enc(x)\n",
    "#         pred = torch.zeros((x.shape[0], 30, 2), device=self.device)\n",
    "#         out = x[:, -1, :].unsqueeze(1)\n",
    "#         for t in range(30):\n",
    "#             out, (h, c) = self.dec(out, (h, c))\n",
    "#             out = self.fc(out)\n",
    "#             pred[:, t, :] = (out.squeeze(1)[:, :2])\n",
    "#         return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path, model, optimizer, scheduler, epoch, loss, batch):\n",
    "    to_save = {\n",
    "        MODEL_STATE: model.state_dict(),\n",
    "        OPTIMIZER_STATE: optimizer.state_dict(),\n",
    "        SCHEDULER_STATE: scheduler.state_dict(),\n",
    "        EPOCH_STATE: epoch,\n",
    "        LOSS_STATE: loss,\n",
    "        BATCH_STATE: batch,        \n",
    "    }\n",
    "    torch.save(to_save, path)\n",
    "    \n",
    "def load_model(path, model, optimizer, scheduler):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint[MODEL_STATE])\n",
    "    optimizer.load_state_dict(checkpoint[OPTIMIZER_STATE])\n",
    "    scheduler.load_state_dict(checkpoint[SCHEDULER_STATE])\n",
    "    return checkpoint[EPOCH_STATE], checkpoint[LOSS_STATE], checkpoint[BATCH_STATE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb5285db542477a8827da8508ddebef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1812171.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706db7a41a8d4dbdbf965d3252f1744d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize datasets and loaders\n",
    "# train_data, test_data, val_data = get_data('single', 'not_extended')\n",
    "train_data, test_data, val_data = get_data('tracked', 'extended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ArgoverseDataset(train_data)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, \n",
    "                                           shuffle=True, collate_fn=collate_train_test, \n",
    "                                           num_workers=N_WORKERS, drop_last=True)\n",
    "test_dataset = ArgoverseDataset(test_data)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST, \n",
    "                                          shuffle=False, collate_fn=collate_train_test, \n",
    "                                          num_workers=N_WORKERS)\n",
    "val_dataset = ArgoverseDataset(val_data)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE_VAL, \n",
    "                                         shuffle=False, collate_fn=collate_val,\n",
    "                                         num_workers=N_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Argo model parameters is 575548\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "model = ArgoNet(DEVICE).to(DEVICE)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "# optimizer = torch.optim.Adagrad(model.parameters())\n",
    "# optimizer = torch.optim.Adadelta(model.parameters())\n",
    "# optimizer = torch.optim.RMSprop(model.parameters())\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, threshold=0.0001)\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.1, step_size_up=600, step_size_down=600)\n",
    "\n",
    "print(f\"Number of Argo model parameters is {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload a model\n",
    "# RELOAD_PATH = \"\"\n",
    "# model = ArgoNet(DEVICE).to(DEVICE)\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10)\n",
    "# _, (train_losses, test_losses), _ = load_model('current_model_storage/.tar', model, optimizer, scheduler)\n",
    "\n",
    "# num_params = sum(p.numel() for p in model.parameters())   \n",
    "# print(f\"Number of model parameters is {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd88e7aa3f94ca4a8183650902b23bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7008.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3686f157c60c4b9c9a7a96ec9d645238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=71.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: train_loss = 1.6984282414918517, test_loss = 1.7233123880337307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833e0328f2444d57a80cfd2e8be45638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcd8874bef24417bba4816f4e3dd325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7008.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8024994aaaf4051bc85be0ea5e9003f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=71.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: train_loss = 1.690778491390391, test_loss = 1.705115380290854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61efaeaaa8d44cdab2903a8bec3da2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc48baf6b904dbeb8375b5ae2b48551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7008.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af1eed8b38e4ac29001dd46876bdd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=71.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: train_loss = 1.6826818389245075, test_loss = 1.7020819781566217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0fa64462a254ac596d474166e26c3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945ae559be5e4ce6b7c7fb3e5d7c6110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7008.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a678c9f5be472a9d534b502fb5db7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=71.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: train_loss = 1.6775050369353284, test_loss = 1.6920467621311028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d693bb61c7549028bd31b33509d5088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9219e72537048f8a341075c782b52bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7008.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6221f2a256354c0293d24d007a3698e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=71.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: train_loss = 1.673590361149872, test_loss = 1.6922676786130764\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b05c82feed4762a460ba234c3d7c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(1, NUM_EPOCH + 1):\n",
    "    train_loss = train(model, DEVICE, train_loader, optimizer, 1, train_data)\n",
    "    test_loss = test(model, DEVICE, test_loader, test_data)      \n",
    "#     scheduler.step(test_loss)  # Uncomment this if using ReduceLROnPLateau\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)        \n",
    "    epoch = len(train_losses)\n",
    "    print(f'Epoch {len(train_losses)}: train_loss = {train_loss}, test_loss = {test_loss}')\n",
    "\n",
    "    save_model(f'report_opt/lstm_adam_lane_epoch{epoch}.tar', model, optimizer, scheduler, \n",
    "               len(train_losses), (train_losses, test_losses), 256)\n",
    "    validate(model, DEVICE, \n",
    "             val_loader, f'report_opt/lstm_adam_lane_epoch{epoch}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(losses):\n",
    "    \"\"\"\n",
    "    Plots the losses over each training iteration. \n",
    "    Assumes that each element of the 'losses' list corresponds to the loss after each batch of train()\n",
    "    \"\"\"\n",
    "    t_iter = np.arange(1, len(losses) + 1, 1, dtype=int)\n",
    "    ax = sns.scatterplot(x=t_iter, y=losses, alpha=0.5)    \n",
    "    ax.set_xlabel('Batch iteration number')\n",
    "    ax.set_ylabel('Root-mean-square loss')\n",
    "    ax.set_title('Batch Iteration vs. Root-Mean-Square Loss')\n",
    "    plt.savefig('lossViter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground Truth Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, device, loader):\n",
    "    \"\"\"\n",
    "    Compares some randomly selected data samples to the model's predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of data\n",
    "    _, (inp, out, scene_idxs, agent_idxs, masks) = next(enumerate(loader))\n",
    "    \n",
    "    # Move tensors to chosen device\n",
    "    inp, out = inp.to(device), out.to(device)\n",
    "    \n",
    "    # Sample number\n",
    "    i = 0\n",
    "    \n",
    "    # Scene idx\n",
    "    scene_idx = scene_idxs[i]\n",
    "        \n",
    "    # Get contiguous arrays of the ground truth output positions\n",
    "    truth = target[i].cpu().detach().numpy()\n",
    "    x = truth[:, 0]\n",
    "    y = truth[:, 1] \n",
    "        \n",
    "    # Get contiguous arrays of the prediction output positions\n",
    "    output = model(inp)    \n",
    "    pred = output[i].cpu().detach().numpy()\n",
    "    xh = pred[:, 0]\n",
    "    yh = pred[:, 0]    \n",
    "    \n",
    "    # Plot the ground truth and prediction positions\n",
    "    fig, (ax) = plt.subplots(nrows=1, ncols=1, figsize=(3, 3))\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('Scene ' + str(scene_idx))\n",
    "    ax.scatter(x, y, label='Ground Truth')\n",
    "    ax.scatter(xh, yh, label='Prediction')\n",
    "    ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
