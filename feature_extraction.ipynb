{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file is used to extract the input/output positions and velocities, lane, and lane norm from the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "import tqdm.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "TEST_PATH = './new_val_in'  # Path to input of the test set\n",
    "TRAIN_PATH = './new_train'  # Path to input + output of training set\n",
    "\n",
    "DUMMY_TRAIN_PATH = './dummy_train'\n",
    "DUMMY_TEST_PATH = './dummy_val'\n",
    "\n",
    "IN_POS_FILE = './input_positions.csv'\n",
    "OUT_POS_FILE = './output_positions.csv'\n",
    "\n",
    "IN_VEL_FILE = './input_velocities.csv'\n",
    "OUT_VEL_FILE = './output_velocities.csv'\n",
    "\n",
    "LANE_FILE = './lane.txt'\n",
    "LANE_NORM_FILE = './lane_norm.txt'\n",
    "\n",
    "IN_POS_FILE_TEST = './input_positions_test.csv'\n",
    "IN_VEL_FILE_TEST = './input_velocities_test.csv'\n",
    "\n",
    "LANE_FILE_TEST = './lane_test.txt'\n",
    "LANE_NORM_FILE_TEST = './lane_norm_test.txt'\n",
    "\n",
    "# Keys to the pickle objects\n",
    "CITY = 'city'\n",
    "LANE = 'lane'\n",
    "LANE_NORM = 'lane_norm'\n",
    "SCENE_IDX = 'scene_idx'\n",
    "AGENT_ID = 'agent_id'\n",
    "P_IN = 'p_in'\n",
    "V_IN = 'v_in'\n",
    "P_OUT = 'p_out'\n",
    "V_OUT = 'v_out'\n",
    "CAR_MASK = 'car_mask'\n",
    "TRACK_ID = 'track_id'\n",
    "\n",
    "# Additional keys for DataFrames\n",
    "WAS_TARGET = 'was_target'\n",
    "P_IN_X = ['p_in_x' + str(i) for i in range(1, 20)]\n",
    "P_IN_Y = ['p_in_y' + str(i) for i in range(1, 20)]\n",
    "V_IN_X = ['v_in_x' + str(i) for i in range(1, 20)]\n",
    "V_IN_Y = ['v_in_y' + str(i) for i in range(1, 20)]\n",
    "\n",
    "P_OUT_X = ['p_out_x' + str(i) for i in range(1, 20)]\n",
    "P_OUT_Y = ['p_out_y' + str(i) for i in range(1, 20)]\n",
    "V_OUT_X = ['v_out_x' + str(i) for i in range(1, 20)]\n",
    "V_OUT_Y = ['v_out_y' + str(i) for i in range(1, 20)]\n",
    "\n",
    "P_IN_CSV_HEADER = [SCENE_IDX, CITY, TRACK_ID, WAS_TARGET] + P_IN_X + P_IN_Y\n",
    "# Add commas for each element\n",
    "P_IN_CSV_HEADER = [col + ',' for col in P_IN_CSV_HEADER]\n",
    "# Remove the last commas\n",
    "P_IN_CSV_HEADER[-1] = P_IN_CSV_HEADER[-1].rstrip(',')\n",
    "\n",
    "V_IN_CSV_HEADER = [SCENE_IDX, CITY, TRACK_ID, WAS_TARGET] + V_IN_X + V_IN_Y\n",
    "# Add commas for each element\n",
    "V_IN_CSV_HEADER = [col + ',' for col in V_IN_CSV_HEADER]\n",
    "# Remove the last commas\n",
    "V_IN_CSV_HEADER[-1] = V_IN_CSV_HEADER[-1].rstrip(',')\n",
    "\n",
    "P_OUT_CSV_HEADER = [SCENE_IDX, CITY, TRACK_ID, WAS_TARGET] + P_OUT_X + P_OUT_Y\n",
    "# Add commas for each element\n",
    "P_OUT_CSV_HEADER = [col + ',' for col in P_OUT_CSV_HEADER]\n",
    "# Remove the last commas\n",
    "P_OUT_CSV_HEADER[-1] = P_OUT_CSV_HEADER[-1].rstrip(',')\n",
    "\n",
    "V_OUT_CSV_HEADER = [SCENE_IDX, CITY, TRACK_ID, WAS_TARGET] + V_OUT_X + V_OUT_Y\n",
    "# Add commas for each element\n",
    "V_OUT_CSV_HEADER = [col + ',' for col in V_OUT_CSV_HEADER]\n",
    "# Remove the last commas\n",
    "V_OUT_CSV_HEADER[-1] = V_OUT_CSV_HEADER[-1].rstrip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training and test paths\n",
    "\n",
    "# train_path = DUMMY_TRAIN_PATH\n",
    "# test_path = DUMMY_TEST_PATH\n",
    "\n",
    "train_path = TRAIN_PATH\n",
    "test_path = TEST_PATH\n",
    "\n",
    "CSV_PATH = IN_VEL_FILE_TEST  # target file \n",
    "CSV_HEADER = V_IN_CSV_HEADER  # target header\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDatasetEDA(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDatasetEDA, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_train_eda(batch):\n",
    "    \"\"\" \n",
    "    Custom collate_fn function for train dataset. \n",
    "        \"\"\"          \n",
    "    batch_data = []\n",
    "    for scene in batch:  \n",
    "        # Get data for tracked agents\n",
    "        idxs = np.nonzero(scene[CAR_MASK])[0]  # indexes of tracked agents out of the 60\n",
    "        scene_idxs = [scene[SCENE_IDX]] * len(idxs)\n",
    "        city = [scene[CITY]] * len(idxs)\n",
    "        track_ids = scene[TRACK_ID][idxs, 0, 0]  # id's of tracked agents\n",
    "        was_target = (track_ids == scene[AGENT_ID]).astype(int)  # whether tracked agent was the target\n",
    "        \n",
    "        # @ CHANGE these lines accordingly\n",
    "#         p_in_x = scene[P_IN][idxs, :, 0]  # all p_in x-components\n",
    "#         p_in_y = scene[P_IN][idxs, :, 1]  # all p_in y-components \n",
    "#         batch_data.append([scene_idxs, city, track_ids, was_target, p_in_x, p_in_y])                   \n",
    "\n",
    "\n",
    "        v_in_x = scene[V_IN][idxs, :, 0]  # all v_in x-components\n",
    "        v_in_y = scene[V_IN][idxs, :, 1]  # all v_in y-components \n",
    "        batch_data.append([scene_idxs, city, track_ids, was_target, v_in_x, v_in_y]) \n",
    "\n",
    "    out = []\n",
    "    return [batch_data, out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_test_eda(batch):\n",
    "    \"\"\" \n",
    "    Custom collate_fn for validation dataset.\n",
    "    \"\"\"    \n",
    "    batch_data = []\n",
    "    for scene in batch:  \n",
    "        # Get data for tracked agents\n",
    "        idxs = np.nonzero(scene[CAR_MASK])[0]  # indexes of tracked agents out of the 60\n",
    "        scene_idxs = [scene[SCENE_IDX]] * len(idxs)\n",
    "        city = [scene[CITY]] * len(idxs)\n",
    "        track_ids = scene[TRACK_ID][idxs, 0, 0]  # id's of tracked agents\n",
    "        was_target = (track_ids == scene[AGENT_ID]).astype(int)  # whether tracked agent was the target\n",
    "        \n",
    "        # @ CHANGE these lines accordingly\n",
    "#         p_in_x = scene[P_IN][idxs, :, 0]  # all p_in x-components\n",
    "#         p_in_y = scene[P_IN][idxs, :, 1]  # all p_in y-components \n",
    "#         batch_data.append([scene_idxs, city, track_ids, was_target, p_in_x, p_in_y])   \n",
    "        \n",
    "        v_in_x = scene[V_IN][idxs, :, 0]  # all v_in x-components\n",
    "        v_in_y = scene[V_IN][idxs, :, 1]  # all v_in y-components         \n",
    "        batch_data.append([scene_idxs, city, track_ids, was_target, v_in_x, v_in_y])                   \n",
    "\n",
    "\n",
    "    out = []\n",
    "    return [batch_data, out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ArgoverseDatasetEDA(data_path=train_path)\n",
    "# loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,shuffle=False, \n",
    "#                                      collate_fn=collate_train_eda, num_workers=NUM_WORKERS)\n",
    "dataset = ArgoverseDatasetEDA(data_path=test_path)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,shuffle=False, \n",
    "                                     collate_fn=collate_test_eda, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look at one data sample      \n",
    "# for i_batch, (data, label) in enumerate(loader):\n",
    "#     print(type(data))\n",
    "#     print(type(label))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_csv(loader):  \n",
    "    \"\"\"\n",
    "    Extracts hard-coded, specified features from Pickle files into CSV file.\n",
    "    \n",
    "    BEFORE calling this method:\n",
    "    1. You'll need to change the CSV_PATH, CSV_HEADER, and scene[4] and scene[5] keys\n",
    "    for each feature you investigate.\n",
    "    \n",
    "    2. Make sure the collate_fn you use has selected the feature you're investigating.\n",
    "    \"\"\"\n",
    "    with open(CSV_PATH, \"w\") as csv_file:\n",
    "        # Clear the csv file before appending data to it\n",
    "        csv_file.truncate()\n",
    "        # Write the header to the csv file\n",
    "        csv_file.writelines(CSV_HEADER + ['\\n']) \n",
    "        \n",
    "    iterator = tqdm.notebook.tqdm(loader, total=int(len(loader)))\n",
    "    \n",
    "    for batch_idx, (data, label) in enumerate(iterator):\n",
    "        # Convert data into DataFrame\n",
    "        for scene in data:\n",
    "            df = pd.DataFrame(\n",
    "                {\n",
    "                    SCENE_IDX: scene[0],\n",
    "                    CITY: scene[1],\n",
    "                    TRACK_ID: scene[2],\n",
    "                    WAS_TARGET: scene[3],                            \n",
    "                }\n",
    "            )\n",
    "            # @ CHANGE these lines accordingly\n",
    "#             df[P_IN_X] = scene[4]\n",
    "#             df[P_IN_Y] = scene[5]\n",
    "            df[V_IN_X] = scene[4]\n",
    "            df[V_IN_Y] = scene[5]\n",
    "            # Write data to CSV file\n",
    "            df.to_csv(CSV_PATH, index=False, header=False, mode='a')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fed99ad14546078a500c46886c121e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_to_csv(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane and lane norm extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lane_to_csv(src_path, out_path, key):\n",
    "    \"\"\"\n",
    "    src_path: directory containing the Picke files to get data from\n",
    "    out_path: file to write the results to\n",
    "    key: One of LANE or LANE_NORM\n",
    "\n",
    "    \"\"\"\n",
    "    # Get all lane components\n",
    "    # Open directory containing pickle files\n",
    "    with open(out_path, \"w\") as output_file:\n",
    "        with os.scandir(src_path) as entries:  \n",
    "            iterator = tqdm.notebook.tqdm(entries, total=3200)        \n",
    "            for entry in iterator:  \n",
    "                # Load the  pickle file\n",
    "                with open(entry, \"rb\") as file:\n",
    "                    scene = pickle.load(file)\n",
    "\n",
    "                    scene_idx = scene[SCENE_IDX]\n",
    "                    n_lanes = scene[key].shape[0]\n",
    "                    xlane = scene[key][:, 0]\n",
    "                    ylane = scene[key][:, 1]\n",
    "                    data = np.concatenate( (xlane, ylane) )\n",
    "\n",
    "                    # Form comma-separated string of all lane values\n",
    "                    # Format is scene index, number of lanes, x-values, y-values\n",
    "                    s = ','.join( [str(scene_idx), str(n_lanes)] + [str(num) for num in data] )                  \n",
    "                    # Write data to file                \n",
    "                    output_file.write(s + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e4fa93cc624db9912ef39d687255f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lane_to_csv(TEST_PATH, LANE_NORM_FILE_TEST, LANE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
