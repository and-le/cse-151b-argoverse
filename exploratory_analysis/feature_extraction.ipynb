{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file is used to extract the input/output positions and velocities, lane, and lane norm from the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm\n",
    "import tqdm.notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "TEST_PATH = './new_val_in'  # Path to input of the test set\n",
    "TRAIN_PATH = './new_train'  # Path to input + output of training set\n",
    "\n",
    "DUMMY_TRAIN_PATH = './dummy_train'\n",
    "DUMMY_TEST_PATH = './dummy_val'\n",
    "\n",
    "IN_POS_FILE = './input_positions.csv'\n",
    "OUT_POS_FILE = './output_positions.csv'\n",
    "\n",
    "IN_VEL_FILE = './input_velocities.csv'\n",
    "OUT_VEL_FILE = './output_velocities.csv'\n",
    "\n",
    "LANE_FILE = './lane.txt'\n",
    "LANE_NORM_FILE = './lane_norm.txt'\n",
    "\n",
    "IN_POS_FILE_TEST = './input_positions_test.csv'\n",
    "IN_VEL_FILE_TEST = './input_velocities_test.csv'\n",
    "\n",
    "LANE_FILE_TEST = './lane_test.txt'\n",
    "LANE_NORM_FILE_TEST = './lane_norm_test.txt'\n",
    "\n",
    "# Keys to the pickle objects\n",
    "CITY = 'city'\n",
    "LANE = 'lane'\n",
    "LANE_NORM = 'lane_norm'\n",
    "SCENE_IDX = 'scene_idx'\n",
    "AGENT_ID = 'agent_id'\n",
    "P_IN = 'p_in'\n",
    "V_IN = 'v_in'\n",
    "P_OUT = 'p_out'\n",
    "V_OUT = 'v_out'\n",
    "CAR_MASK = 'car_mask'\n",
    "TRACK_ID = 'track_id'\n",
    "\n",
    "# Additional keys for DataFrames\n",
    "WAS_TARGET = 'was_target'\n",
    "P_IN_X = ['p_in_x' + str(i) for i in range(1, 20)]\n",
    "P_IN_Y = ['p_in_y' + str(i) for i in range(1, 20)]\n",
    "V_IN_X = ['v_in_x' + str(i) for i in range(1, 20)]\n",
    "V_IN_Y = ['v_in_y' + str(i) for i in range(1, 20)]\n",
    "\n",
    "P_OUT_X = ['p_out_x' + str(i) for i in range(1, 20)]\n",
    "P_OUT_Y = ['p_out_y' + str(i) for i in range(1, 20)]\n",
    "V_OUT_X = ['v_out_x' + str(i) for i in range(1, 20)]\n",
    "V_OUT_Y = ['v_out_y' + str(i) for i in range(1, 20)]\n",
    "\n",
    "P_IN_CSV_HEADER = [SCENE_IDX, CITY, TRACK_ID, WAS_TARGET] + P_IN_X + P_IN_Y\n",
    "# Add commas for each element\n",
    "P_IN_CSV_HEADER = [col + ',' for col in P_IN_CSV_HEADER]\n",
    "# Remove the last commas\n",
    "P_IN_CSV_HEADER[-1] = P_IN_CSV_HEADER[-1].rstrip(',')\n",
    "\n",
    "V_IN_CSV_HEADER = [SCENE_IDX, CITY, TRACK_ID, WAS_TARGET] + V_IN_X + V_IN_Y\n",
    "# Add commas for each element\n",
    "V_IN_CSV_HEADER = [col + ',' for col in V_IN_CSV_HEADER]\n",
    "# Remove the last commas\n",
    "V_IN_CSV_HEADER[-1] = V_IN_CSV_HEADER[-1].rstrip(',')\n",
    "\n",
    "P_OUT_CSV_HEADER = [SCENE_IDX, CITY, TRACK_ID, WAS_TARGET] + P_OUT_X + P_OUT_Y\n",
    "# Add commas for each element\n",
    "P_OUT_CSV_HEADER = [col + ',' for col in P_OUT_CSV_HEADER]\n",
    "# Remove the last commas\n",
    "P_OUT_CSV_HEADER[-1] = P_OUT_CSV_HEADER[-1].rstrip(',')\n",
    "\n",
    "V_OUT_CSV_HEADER = [SCENE_IDX, CITY, TRACK_ID, WAS_TARGET] + V_OUT_X + V_OUT_Y\n",
    "# Add commas for each element\n",
    "V_OUT_CSV_HEADER = [col + ',' for col in V_OUT_CSV_HEADER]\n",
    "# Remove the last commas\n",
    "V_OUT_CSV_HEADER[-1] = V_OUT_CSV_HEADER[-1].rstrip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training and test paths\n",
    "\n",
    "# train_path = DUMMY_TRAIN_PATH\n",
    "# test_path = DUMMY_TEST_PATH\n",
    "\n",
    "train_path = TRAIN_PATH\n",
    "test_path = TEST_PATH\n",
    "\n",
    "CSV_PATH = IN_VEL_FILE_TEST  # target file \n",
    "CSV_HEADER = V_IN_CSV_HEADER  # target header\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDatasetEDA(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDatasetEDA, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_train_eda(batch):\n",
    "    \"\"\" \n",
    "    Custom collate_fn function for train dataset. \n",
    "        \"\"\"          \n",
    "    batch_data = []\n",
    "    for scene in batch:  \n",
    "        # Get data for tracked agents\n",
    "        idxs = np.nonzero(scene[CAR_MASK])[0]  # indexes of tracked agents out of the 60\n",
    "        scene_idxs = [scene[SCENE_IDX]] * len(idxs)\n",
    "        city = [scene[CITY]] * len(idxs)\n",
    "        track_ids = scene[TRACK_ID][idxs, 0, 0]  # id's of tracked agents\n",
    "        was_target = (track_ids == scene[AGENT_ID]).astype(int)  # whether tracked agent was the target\n",
    "        \n",
    "        # @ CHANGE these lines accordingly\n",
    "#         p_in_x = scene[P_IN][idxs, :, 0]  # all p_in x-components\n",
    "#         p_in_y = scene[P_IN][idxs, :, 1]  # all p_in y-components \n",
    "#         batch_data.append([scene_idxs, city, track_ids, was_target, p_in_x, p_in_y])                   \n",
    "\n",
    "\n",
    "        v_in_x = scene[V_IN][idxs, :, 0]  # all v_in x-components\n",
    "        v_in_y = scene[V_IN][idxs, :, 1]  # all v_in y-components \n",
    "        batch_data.append([scene_idxs, city, track_ids, was_target, v_in_x, v_in_y]) \n",
    "\n",
    "    out = []\n",
    "    return [batch_data, out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_test_eda(batch):\n",
    "    \"\"\" \n",
    "    Custom collate_fn for validation dataset.\n",
    "    \"\"\"    \n",
    "    batch_data = []\n",
    "    for scene in batch:  \n",
    "        # Get data for tracked agents\n",
    "        idxs = np.nonzero(scene[CAR_MASK])[0]  # indexes of tracked agents out of the 60\n",
    "        scene_idxs = [scene[SCENE_IDX]] * len(idxs)\n",
    "        city = [scene[CITY]] * len(idxs)\n",
    "        track_ids = scene[TRACK_ID][idxs, 0, 0]  # id's of tracked agents\n",
    "        was_target = (track_ids == scene[AGENT_ID]).astype(int)  # whether tracked agent was the target\n",
    "        \n",
    "        # @ CHANGE these lines accordingly\n",
    "#         p_in_x = scene[P_IN][idxs, :, 0]  # all p_in x-components\n",
    "#         p_in_y = scene[P_IN][idxs, :, 1]  # all p_in y-components \n",
    "#         batch_data.append([scene_idxs, city, track_ids, was_target, p_in_x, p_in_y])   \n",
    "        \n",
    "        v_in_x = scene[V_IN][idxs, :, 0]  # all v_in x-components\n",
    "        v_in_y = scene[V_IN][idxs, :, 1]  # all v_in y-components         \n",
    "        batch_data.append([scene_idxs, city, track_ids, was_target, v_in_x, v_in_y])                   \n",
    "\n",
    "\n",
    "    out = []\n",
    "    return [batch_data, out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = ArgoverseDatasetEDA(data_path=train_path)\n",
    "# loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,shuffle=False, \n",
    "#                                      collate_fn=collate_train_eda, num_workers=NUM_WORKERS)\n",
    "dataset = ArgoverseDatasetEDA(data_path=test_path)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,shuffle=False, \n",
    "                                     collate_fn=collate_test_eda, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look at one data sample      \n",
    "# for i_batch, (data, label) in enumerate(loader):\n",
    "#     print(type(data))\n",
    "#     print(type(label))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_csv(loader):  \n",
    "    \"\"\"\n",
    "    Extracts hard-coded, specified features from Pickle files into CSV file.\n",
    "    \n",
    "    BEFORE calling this method:\n",
    "    1. You'll need to change the CSV_PATH, CSV_HEADER, and scene[4] and scene[5] keys\n",
    "    for each feature you investigate.\n",
    "    \n",
    "    2. Make sure the collate_fn you use has selected the feature you're investigating.\n",
    "    \"\"\"\n",
    "    with open(CSV_PATH, \"w\") as csv_file:\n",
    "        # Clear the csv file before appending data to it\n",
    "        csv_file.truncate()\n",
    "        # Write the header to the csv file\n",
    "        csv_file.writelines(CSV_HEADER + ['\\n']) \n",
    "        \n",
    "    iterator = tqdm.notebook.tqdm(loader, total=int(len(loader)))\n",
    "    \n",
    "    for batch_idx, (data, label) in enumerate(iterator):\n",
    "        # Convert data into DataFrame\n",
    "        for scene in data:\n",
    "            df = pd.DataFrame(\n",
    "                {\n",
    "                    SCENE_IDX: scene[0],\n",
    "                    CITY: scene[1],\n",
    "                    TRACK_ID: scene[2],\n",
    "                    WAS_TARGET: scene[3],                            \n",
    "                }\n",
    "            )\n",
    "            # @ CHANGE these lines accordingly\n",
    "#             df[P_IN_X] = scene[4]\n",
    "#             df[P_IN_Y] = scene[5]\n",
    "            df[V_IN_X] = scene[4]\n",
    "            df[V_IN_Y] = scene[5]\n",
    "            # Write data to CSV file\n",
    "            df.to_csv(CSV_PATH, index=False, header=False, mode='a')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fed99ad14546078a500c46886c121e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_to_csv(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lane and lane norm extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = './original_train'\n",
    "# path_mia_x = './mia_lane_x.txt'\n",
    "# path_pit_x = './pit_lane_x.txt'\n",
    "# path_mia_y = './mia_lane_y.txt'\n",
    "# path_pit_y = './pit_lane_y.txt'\n",
    "# key = LANE\n",
    "\n",
    "path_mia_x = './mia_lanenorm_x.txt'\n",
    "path_pit_x = './pit_lanenorm_x.txt'\n",
    "path_mia_y = './mia_lanenorm_y.txt'\n",
    "path_pit_y = './pit_lanenorm_y.txt'\n",
    "key = LANE_NORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d6442052b3467b888ee45b66a816cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=205942.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(path_mia_x, \"w\") as mia_x, open(path_pit_x, 'w') as pit_x, open(path_mia_y, 'w') as mia_y, open(path_pit_y, 'w') as pit_y:\n",
    "    with os.scandir(src_path) as entries:  \n",
    "        iterator = tqdm.notebook.tqdm(entries, total=205942)        \n",
    "        for entry in iterator:  \n",
    "            # Load the  pickle file\n",
    "            with open(entry, \"rb\") as file:\n",
    "                scene = pickle.load(file)\n",
    "                scene_idx = scene[SCENE_IDX]\n",
    "                city = scene[CITY]\n",
    "                xlane = scene[key][:, 0]\n",
    "                ylane = scene[key][:, 1]\n",
    "\n",
    "                # Form comma-separated string of all lane values\n",
    "                sx = ','.join( [str(scene_idx)] + [str(num) for num in xlane] )\n",
    "                sy = ','.join( [str(scene_idx)] + [str(num) for num in ylane] )                  \n",
    "\n",
    "                # Write data to file \n",
    "                if city == 'MIA':\n",
    "                    mia_x.write(sx + '\\n')\n",
    "                    mia_y.write(sy + '\\n')\n",
    "                else:\n",
    "                    pit_x.write(sx + '\\n')\n",
    "                    pit_y.write(sy + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abf44eae2334cc9a2959ea4f7710efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=205942.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(path_mia_y, 'w') as mia_y, open(path_pit_y, 'w') as pit_y:\n",
    "    with os.scandir(src_path) as entries:  \n",
    "        iterator = tqdm.notebook.tqdm(entries, total=205942)        \n",
    "        for entry in iterator:  \n",
    "            # Load the  pickle file\n",
    "            with open(entry, \"rb\") as file:\n",
    "                scene = pickle.load(file)\n",
    "                scene_idx = scene[SCENE_IDX]\n",
    "                city = scene[CITY]\n",
    "                ylane = scene[key][:, 1]\n",
    "\n",
    "                # Form comma-separated string of all lane values\n",
    "                sy = ','.join( [str(scene_idx)] + [str(num) for num in ylane] )                  \n",
    "\n",
    "                # Write data to file \n",
    "                if city == 'MIA':\n",
    "                    mia_y.write(sy + '\\n')\n",
    "                else:\n",
    "                    pit_y.write(sy + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified data creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_HEADER = ['x' + str(i // 2 + 1) if i % 2 == 0 else 'y' + str(i // 2 + 1) for i in range(38)]\n",
    "OUT_HEADER = ['x' + str(i // 2 + 1) if i % 2 == 0 else 'y' + str(i // 2 + 1) for i in range(60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_HEADER = [SCENE_IDX] + IN_HEADER\n",
    "IN_HEADER = [col + ',' for col in IN_HEADER]\n",
    "IN_HEADER[-1] = IN_HEADER[-1].rstrip(',')\n",
    "\n",
    "OUT_HEADER = [SCENE_IDX] + OUT_HEADER\n",
    "OUT_HEADER = [col + ',' for col in OUT_HEADER]\n",
    "OUT_HEADER[-1] = OUT_HEADER[-1].rstrip(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e21bdb25dbd46e787664f552f3d26f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=205942.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Target agents version\n",
    "with open('./pin_train.csv', \"w\") as fpin, open('./pout_train.csv', 'w') as fpout, open('./vin_train.csv', 'w') as fvin, open('./vout_train.csv', 'w') as fvout:\n",
    "    # Write headers to the files\n",
    "    fpin.writelines(IN_HEADER + ['\\n']) \n",
    "    fpout.writelines(OUT_HEADER + ['\\n']) \n",
    "    fvin.writelines(IN_HEADER + ['\\n']) \n",
    "    fvout.writelines(OUT_HEADER + ['\\n']) \n",
    "\n",
    "    with os.scandir('./original_train') as entries:  \n",
    "        iterator = tqdm.notebook.tqdm(entries, total=205942)        \n",
    "        for entry in iterator:  \n",
    "            # Load the  pickle file\n",
    "            with open(entry, \"rb\") as file:\n",
    "                scene = pickle.load(file)\n",
    "                scene_idx = scene[SCENE_IDX]\n",
    "                # Get the target agent id\n",
    "                agent_id = scene[AGENT_ID]        \n",
    "                # Get the matrix of all agents\n",
    "                track_id = scene[TRACK_ID]        \n",
    "                # Get the location of the target agent in the matrix\n",
    "                idx = np.nonzero(track_id[:, 0] == agent_id)[0][0]\n",
    "                \n",
    "                pin, pout, vin, vout = scene[P_IN][idx], scene[P_OUT][idx], scene[V_IN][idx], scene[V_OUT][idx]        \n",
    "                pin = pin.flatten()\n",
    "                pout = pout.flatten()\n",
    "                vin = vin.flatten()\n",
    "                vout = vout.flatten()\n",
    "\n",
    "                # Form comma-separated string of all values\n",
    "                spin = ','.join( [str(scene_idx)] + [str(p) for p in pin] )\n",
    "                spout = ','.join( [str(scene_idx)] + [str(p) for p in pout] )\n",
    "                svin = ','.join( [str(scene_idx)] + [str(p) for p in vin] )                  \n",
    "                svout = ','.join( [str(scene_idx)] + [str(p) for p in vout] )\n",
    "                                \n",
    "                # Write data to file \n",
    "                fpin.write(spin + '\\n')\n",
    "                fpout.write(spout + '\\n')\n",
    "                fvin.write(svin + '\\n')\n",
    "                fvout.write(svout + '\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84677dc2ebc4cf1aac3ef57734dfd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=205942.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# All non-dummy agents version\n",
    "with open('./pin_trainall.csv', \"w\") as fpin, open('./pout_trainall.csv', 'w') as fpout, open('./vin_trainall.csv', 'w') as fvin, open('./vout_trainall.csv', 'w') as fvout:\n",
    "    # Write headers to the files\n",
    "    fpin.writelines(IN_HEADER + ['\\n']) \n",
    "    fpout.writelines(OUT_HEADER + ['\\n']) \n",
    "    fvin.writelines(IN_HEADER + ['\\n']) \n",
    "    fvout.writelines(OUT_HEADER + ['\\n']) \n",
    "\n",
    "    with os.scandir('./original_train') as entries:  \n",
    "        iterator = tqdm.notebook.tqdm(entries, total=205942)        \n",
    "        for entry in iterator:  \n",
    "            # Load the  pickle file\n",
    "            with open(entry, \"rb\") as file:\n",
    "                scene = pickle.load(file)\n",
    "                scene_idx = scene[SCENE_IDX]\n",
    "                idx = int(np.sum(scene[CAR_MASK]))               \n",
    "                \n",
    "                pin, pout, vin, vout = scene[P_IN][:idx], scene[P_OUT][:idx], scene[V_IN][:idx], scene[V_OUT][:idx] \n",
    "                for i in range(idx):\n",
    "                    # Form comma-separated string of all values\n",
    "                    spin = ','.join( [str(scene_idx)] + [str(p) for p in pin[i].flatten()] )\n",
    "                    spout = ','.join( [str(scene_idx)] + [str(p) for p in pout[i].flatten()] )\n",
    "                    svin = ','.join( [str(scene_idx)] + [str(p) for p in vin[i].flatten()] )                  \n",
    "                    svout = ','.join( [str(scene_idx)] + [str(p) for p in vout[i].flatten()] )\n",
    "                                \n",
    "                    # Write data to file \n",
    "                    fpin.write(spin + '\\n')\n",
    "                    fpout.write(spout + '\\n')\n",
    "                    fvin.write(svin + '\\n')\n",
    "                    fvout.write(svout + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ec98e805934034824f037db28bf57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Target agents version\n",
    "with open('./pin_val.csv', \"w\") as fpin, open('./pout_val.csv', 'w') as fpout, open('./vin_val.csv', 'w') as fvin, open('./vout_val.csv', 'w') as fvout:\n",
    "    # Write headers to the files\n",
    "    fpin.writelines(IN_HEADER + ['\\n']) \n",
    "    fpout.writelines(OUT_HEADER + ['\\n']) \n",
    "    fvin.writelines(IN_HEADER + ['\\n']) \n",
    "    fvout.writelines(OUT_HEADER + ['\\n']) \n",
    "\n",
    "    with os.scandir('./val_data') as entries:  \n",
    "        iterator = tqdm.notebook.tqdm(entries, total=3200)        \n",
    "        for entry in iterator:  \n",
    "            # Load the  pickle file\n",
    "            with open(entry, \"rb\") as file:\n",
    "                scene = pickle.load(file)\n",
    "                scene_idx = scene[SCENE_IDX]\n",
    "                # Get the target agent id\n",
    "                agent_id = scene[AGENT_ID]        \n",
    "                # Get the matrix of all agents\n",
    "                track_id = scene[TRACK_ID]        \n",
    "                # Get the location of the target agent in the matrix\n",
    "                idx = np.nonzero(track_id[:, 0] == agent_id)[0][0]\n",
    "                \n",
    "                pin, vin= scene[P_IN][idx], scene[V_IN][idx]        \n",
    "                pin = pin.flatten()\n",
    "                vin = vin.flatten()\n",
    "\n",
    "                # Form comma-separated string of all values\n",
    "                spin = ','.join( [str(scene_idx)] + [str(p) for p in pin] )\n",
    "                svin = ','.join( [str(scene_idx)] + [str(p) for p in vin] )                  \n",
    "                                \n",
    "                # Write data to file \n",
    "                fpin.write(spin + '\\n')\n",
    "                fvin.write(svin + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lane extraction\n",
    "CLS_LANE_IN = 'closest_lanes_in'\n",
    "CLS_LANE_NORM_IN = 'closest_lane_norms_in'\n",
    "CLS_LANE_OUT = 'closest_lanes_out'\n",
    "CLS_LANE_NORM_OUT = 'closest_lane_norms_out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b966183ecc07478f8038044c10a96cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=205942.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# All non-dummy agents version\n",
    "with open('./lanein.csv', \"w\") as flin, open('./normin.csv', 'w') as fnin, open('./laneout.csv', \"w\") as flout, open('./normout.csv', \"w\") as fnout:\n",
    "    # Write headers to the files\n",
    "    flin.writelines(IN_HEADER + ['\\n']) \n",
    "    fnin.writelines(IN_HEADER + ['\\n']) \n",
    "    flout.writelines(OUT_HEADER + ['n'])\n",
    "    fnout.writelines(OUT_HEADER + ['n'])\n",
    "\n",
    "    with os.scandir('./transformed_train_data') as entries:  \n",
    "        iterator = tqdm.notebook.tqdm(entries, total=205942)        \n",
    "        for entry in iterator:  \n",
    "            # Load the  pickle file\n",
    "            with open(entry, \"rb\") as file:\n",
    "                scene = pickle.load(file)\n",
    "                scene_idx = scene[SCENE_IDX]\n",
    "                idx = int(np.sum(scene[CAR_MASK]))               \n",
    "                \n",
    "                lin, nin, = scene[CLS_LANE_IN][:idx], scene[CLS_LANE_NORM_IN][:idx]\n",
    "                lout, nout = scene[CLS_LANE_OUT][:idx], scene[CLS_LANE_NORM_OUT][:idx] \n",
    "                for i in range(idx):\n",
    "                    # Form comma-separated string of all values\n",
    "                    slin = ','.join( [str(scene_idx)] + [str(p) for p in lin[i].flatten()] )\n",
    "                    snin = ','.join( [str(scene_idx)] + [str(p) for p in nin[i].flatten()] )\n",
    "                    slout = ','.join( [str(scene_idx)] + [str(p) for p in lout[i].flatten()] )                  \n",
    "                    snout = ','.join( [str(scene_idx)] + [str(p) for p in nout[i].flatten()] )\n",
    "                                \n",
    "                    # Write data to file \n",
    "                    flin.write(slin + '\\n')\n",
    "                    fnin.write(snin + '\\n')\n",
    "                    flout.write(slout + '\\n')\n",
    "                    fnout.write(snout + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdc5fb9a05d4b2fbbe9d53e59c39f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=3200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# All non-dummy agents version\n",
    "with open('targ_val_data/lanein.csv', \"w\") as flin, open('targ_val_data/normin.csv', 'w') as fnin:\n",
    "    # Write headers to the files\n",
    "    flin.writelines(IN_HEADER + ['\\n']) \n",
    "    fnin.writelines(IN_HEADER + ['\\n']) \n",
    "\n",
    "    with os.scandir('./transformed_val_data') as entries:  \n",
    "        iterator = tqdm.notebook.tqdm(entries, total=3200)        \n",
    "        for entry in iterator:  \n",
    "            # Load the  pickle file\n",
    "            with open(entry, \"rb\") as file:\n",
    "                scene = pickle.load(file)\n",
    "                scene_idx = scene[SCENE_IDX]\n",
    "                idx = int(np.sum(scene[CAR_MASK]))               \n",
    "                \n",
    "                lin, nin, = scene[CLS_LANE_IN][:idx], scene[CLS_LANE_NORM_IN][:idx]\n",
    "                for i in range(idx):\n",
    "                    # Form comma-separated string of all values\n",
    "                    slin = ','.join( [str(scene_idx)] + [str(p) for p in lin[i].flatten()] )\n",
    "                    snin = ','.join( [str(scene_idx)] + [str(p) for p in nin[i].flatten()] )\n",
    "                                \n",
    "                    # Write data to file \n",
    "                    flin.write(slin + '\\n')\n",
    "                    fnin.write(snin + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
